{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54733696-f68e-428b-8a21-a5d4f36c5c7f",
   "metadata": {},
   "source": [
    "# Music Recommendation Service\n",
    "\n",
    "## Summary\n",
    "\n",
    "This Notebook uses a library of 11k songs (7GB of song data) to build an ephemeral music recommendation service. As each cell gets executed you learn about how to generate embeddings for each song which together with Qdrant's vector DB will form the backend of of our service. There is no GUI beyond the Jupyter environment. **No code edits should be required.**\n",
    "\n",
    "\n",
    "## Usage Details\n",
    "\n",
    "The web interface is used to create and edit notebooks, which are documents containing live code, equations, visualizations, and narrative text. The kernel is the computational engine that executes \n",
    "the code in the notebook. When you run a code cell in a notebook, the code is sent to the kernel for execution, and the output is returned to the notebook. **Run a cell by clicking on it and pressing \"Shift + Enter\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff498b-158f-4435-baa4-ba3e1883adad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Start download and importing needed modules and models\n",
    "\n",
    "Note: This code will produce some output to inform you about the state of the model download. GPU support as well as CPU hardware feature support. These will be highlighted in red,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9d42d-0510-4c08-9529-1edfacbf29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the needed modules\n",
    "from IPython.display import Audio as player\n",
    "from datasets import load_dataset, load_from_disk, Audio\n",
    "from panns_inference import AudioTagging\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import openl3\n",
    "import torch\n",
    "import os\n",
    "MUSIC_COLLECTION_DB = \"my_collection\"\n",
    "\n",
    "# get the hostname from OS ENV\n",
    "QDRANT_HOST = os.environ.get('QDRANT_HOST')\n",
    "# connect to Qdrant vector DB\n",
    "client = QdrantClient(host=QDRANT_HOST, port=6333)\n",
    "print(\"Attempting to connect to %s\" % QDRANT_HOST)\n",
    "# check if our collection already exists\n",
    "collections = client.get_collections()\n",
    "music_collection_exits = False\n",
    "if collections:  \n",
    "    if len(collections.collections) > 0 and collections.collections[0].name == MUSIC_COLLECTION_DB:\n",
    "        print(\"%s exists...\" % MUSIC_COLLECTION_DB)\n",
    "        music_collection_exists = True\n",
    "    else:\n",
    "        print(\"%s doesn't exist...\" % MUSIC_COLLECTION_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d533a-607e-45a5-8f98-a1c671c949a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aquire the data we need here for the next steps\n",
    "music_data = load_from_disk(\"./data/complete_music_data_set.arrow\")\n",
    "metadata = pd.read_json(\"./data/metatdata_complete_music_data_set.json\")\n",
    "payload = metadata.to_dict(orient=\"records\")\n",
    "\n",
    "# Note:\n",
    "# We have roughly 11k songs and embedding creation takes 5-10 seconds per song\n",
    "# this would take 15-30h on a single CPU\n",
    "# if you don't have a GPU available you may want to filter the dataset by genre\n",
    "my_genre = \"electronic\"\n",
    "# INPUT: Enable (set FILTER = True) or disable (FILTER = False (default)) the filter here\n",
    "FILTER = False\n",
    "def filter_songs(row):\n",
    "    index = row['index']\n",
    "    return metadata[(metadata[\"index\"] == index) & (metadata['genre'] == my_genre)].empty\n",
    "\n",
    "if FILTER:\n",
    "    subset_meta = metadata[metadata['genre'] == 'electronic'][\"index\"]\n",
    "    subset_music = music_data.filter(filter_songs)\n",
    "    music_data = subset_music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b54d00-3c73-4f3f-9e5d-f79742225a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out one of the songs\n",
    "a_song = music_data[6623]\n",
    "from json import dumps\n",
    "for key, data in a_song.items():\n",
    "    print(f\"{key}: {data}\")\n",
    "\n",
    "sample_rate = a_song['audio']['sampling_rate']\n",
    "player(a_song['audio']['array'], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41245261-3b34-4cdd-8146-89a6cea3847f",
   "metadata": {},
   "source": [
    "### Create our embeddings with either CPU or GPU\n",
    "\n",
    "This code section is written in Python and uses the PyTorch library to create embeddings \n",
    "for a batch of songs. The code first determines whether to use the CPU or GPU for execution \n",
    "based on the availability of a CUDA-enabled GPU. It then defines a helper function that \n",
    "takes a batch of songs and creates embeddings for each song using the AudioTagging model. \n",
    "The embeddings are then added to the batch and returned. Finally, the code checks if the \n",
    "embeddings have already been computed for the music data and, if not, computes them using \n",
    "the get_panns_embs function. This code section is useful for generating embeddings for a \n",
    "large dataset of songs and can be optimized for GPU execution to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434e4a9-18fb-43fa-94ce-803c37f290be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine if we can use cuda or need to fallback to the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# create a helper function which takes a batch of songs and then creates embeddings for each\n",
    "def get_panns_embs(batch):\n",
    "    # CPU execution\n",
    "    if device.type == \"cpu\":\n",
    "        arrays = [torch.tensor(val['array'], dtype=torch.double) for val in batch['audio']]\n",
    "        # padding might not be needed for CPU execution\n",
    "        inputs = torch.nn.utils.rnn.pad_sequence(arrays, batch_first=True, padding_value=0)\n",
    "        inputs = inputs.numpy()\n",
    "    # GPU execution\n",
    "    else:\n",
    "        arrays = [torch.tensor(val['array'], dtype=torch.float64) for val in batch['audio']]\n",
    "        inputs = torch.nn.utils.rnn.pad_sequence(arrays, batch_first=True, padding_value=0).type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    # get the embeddings from the model\n",
    "    _, embedding = at.inference(inputs)\n",
    "    batch['panns_embeddings'] = embedding\n",
    "    return batch\n",
    "\n",
    "# Expensive: only run this if we haven't in the past\n",
    "if not \"panns_embeddings\" in music_data.features:\n",
    "    at = AudioTagging(checkpoint_path=None, device=device.type)\n",
    "    music_data = music_data.map(get_panns_embs, batched=True, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b8d44",
   "metadata": {},
   "source": [
    "### Step here, we will not wait to complete the embedding generation since it takes too long. A \"TV kitchen\" moment will take us to the next step. Return to the instructions now to learn how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b64a71-a805-4d6a-850c-5d3c7d514766",
   "metadata": {},
   "source": [
    "### Connect and store our Embeddings in Qdrant's vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e537035-b2df-4c4f-9997-888bb72ebc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if the collection exists before we recreate it\n",
    "if not music_collection_exits:\n",
    "    client.recreate_collection(\n",
    "        collection_name=MUSIC_COLLECTION_DB,\n",
    "        vectors_config=models.VectorParams(size=2048, distance=models.Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# store the embeddings in the vector DB now so we can start using them in various ways\n",
    "client.upsert(\n",
    "    collection_name=MUSIC_COLLECTION_DB,\n",
    "    points=models.Batch(\n",
    "        ids=music_data['index'],\n",
    "        vectors=music_data['panns_embeddings'],\n",
    "        payloads=payload\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
